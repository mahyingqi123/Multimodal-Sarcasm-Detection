{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is to extract the features from the audio files into embeddings \n",
    "\n",
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016e3ae8e7d6411485b2e4a80fd7414c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yingq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yingq\\.cache\\huggingface\\hub\\models--MIT--ast-finetuned-audioset-10-10-0.4593. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd70e2dea914c4a95d3478a8a25e5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a5d3973cfc4189a438f933b65fde5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 285/690 [11:28<12:44,  1.89s/it]c:\\Users\\yingq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\yingq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 690/690 [18:52<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from transformers import ASTFeatureExtractor, ASTModel\n",
    "\n",
    "# Load AST model and feature extractor\n",
    "model_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(model_name)\n",
    "model = ASTModel.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "path_name = 'audio/final'\n",
    "files = os.listdir(path_name)\n",
    "\n",
    "# Hyperparameters for windowing\n",
    "window_size = 2  # Window size in seconds\n",
    "stride = 1  # Stride in seconds\n",
    "\n",
    "result = {}\n",
    "\n",
    "for file in tqdm(files):\n",
    "    audio_file = os.path.join(path_name, file)\n",
    "    file_id = file.split('.')[0]\n",
    "    \n",
    "    # Load audio file\n",
    "    input_audio, sample_rate = librosa.load(audio_file, sr=16000)\n",
    "    total_duration = librosa.get_duration(y=input_audio, sr=sample_rate)\n",
    "    \n",
    "    # Compute window parameters\n",
    "    window_length = window_size * sample_rate\n",
    "    stride_length = stride * sample_rate\n",
    "    \n",
    "    # Split audio into windows\n",
    "    windows = []\n",
    "    for start in range(0, len(input_audio) - window_length + 1, stride_length):\n",
    "        end = start + window_length\n",
    "        windows.append(input_audio[start:end])\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    # Process each window\n",
    "    for window in windows:\n",
    "        # Extract features using AST feature extractor\n",
    "        inputs = feature_extractor(\n",
    "            window, \n",
    "            sampling_rate=sample_rate,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Extract features using the AST model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Get the mean-pooled embedding for this window\n",
    "            # AST outputs last_hidden_state of shape (batch_size, sequence_length, hidden_size)\n",
    "            window_embedding = torch.mean(outputs.last_hidden_state, dim=1).squeeze().cpu().numpy()\n",
    "            embeddings.append(window_embedding)\n",
    "\n",
    "    # Mean pool across all window embeddings for a global feature vector\n",
    "    mean_pooled_embedding = np.mean(embeddings, axis=0).tolist()\n",
    "    \n",
    "    # Store the result for this audio file\n",
    "    result[file_id] = mean_pooled_embedding\n",
    "\n",
    "# Save the extracted features\n",
    "with open('audio_features_ast.json', 'w') as f:\n",
    "    json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 690/690 [03:01<00:00,  3.79it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import opensmile\n",
    "\n",
    "# Initialize openSMILE with a desired feature set and level.\n",
    "# Here we use the ComParE_2016 feature set with Functionals (which returns a single vector per signal).\n",
    "smile = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "    feature_level=opensmile.FeatureLevel.Functionals,\n",
    ")\n",
    "\n",
    "path_name = 'audio/final'\n",
    "files = os.listdir(path_name)\n",
    "\n",
    "# Hyperparameters for windowing\n",
    "window_size = 2  # Window size in seconds\n",
    "stride = 1       # Stride in seconds\n",
    "\n",
    "result = {}\n",
    "\n",
    "for file in tqdm(files):\n",
    "    audio_file = os.path.join(path_name, file)\n",
    "    file_id = os.path.splitext(file)[0]  # file name without extension\n",
    "    \n",
    "    # Load audio file (resampled to 16kHz)\n",
    "    input_audio, sample_rate = librosa.load(audio_file, sr=16000)\n",
    "    total_duration = librosa.get_duration(y=input_audio, sr=sample_rate)\n",
    "    \n",
    "    # Calculate window and stride lengths in samples\n",
    "    window_length = int(window_size * sample_rate)\n",
    "    stride_length = int(stride * sample_rate)\n",
    "    \n",
    "    # Split audio into overlapping windows\n",
    "    windows = []\n",
    "    for start in range(0, len(input_audio) - window_length + 1, stride_length):\n",
    "        end = start + window_length\n",
    "        windows.append(input_audio[start:end])\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    # Process each window with openSMILE\n",
    "    for window in windows:\n",
    "        # Process the window to extract features; returns a DataFrame with one row\n",
    "        features_df = smile.process_signal(window, sample_rate)\n",
    "        # Extract the feature vector from the DataFrame\n",
    "        window_features = features_df.iloc[0].values.astype(float)\n",
    "        embeddings.append(window_features)\n",
    "    \n",
    "    # Aggregate window embeddings (mean pooling) into a single global feature vector\n",
    "    if embeddings:\n",
    "        mean_pooled_embedding = np.mean(embeddings, axis=0).tolist()\n",
    "    else:\n",
    "        mean_pooled_embedding = None  # or handle files shorter than window_size appropriately\n",
    "    \n",
    "    # Store the result for this audio file\n",
    "    result[file_id] = mean_pooled_embedding\n",
    "\n",
    "# Save the extracted features to a JSON file\n",
    "with open('audio_features_opensmile.json', 'w') as f:\n",
    "    json.dump(result, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('audio_features_opensmile.json', 'w') as f:\n",
    "    json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in result.items():\n",
    "    if len(val) != 6373:\n",
    "        print(key, len(val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
