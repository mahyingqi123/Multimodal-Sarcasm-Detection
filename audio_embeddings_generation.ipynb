{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is to extract the features from the audio files into embeddings \n",
    "\n",
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def getMaximumScore(arr, k):\n",
    "    total = []\n",
    "\n",
    "    for val in arr:\n",
    "        current = val\n",
    "        count = 0\n",
    "        while current >0 and count < k:\n",
    "            total.append(current)\n",
    "            next_val = math.ceil(current/3)\n",
    "            if next_val == current:\n",
    "                break\n",
    "            current = next_val\n",
    "            count += 1\n",
    "    total = sorted(total, reverse=True)\n",
    "    result = sum(total[:k])\n",
    "    return result\n",
    "\n",
    "arr = [4,4,5,18,1]\n",
    "k=3\n",
    "print(getMaximumScore(arr, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCount(binary):\n",
    "    x = set()\n",
    "    y = set()\n",
    "    for i in binary:\n",
    "        new = set()\n",
    "        for j in y:\n",
    "            if j =='0':\n",
    "                num = i\n",
    "            else:\n",
    "                num = j + i\n",
    "            new.add(num)\n",
    "        new.add(i)\n",
    "        y.update(new)\n",
    "        x.update(y)\n",
    "    return len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countDiceSequences(N, rollMax):\n",
    "    max_val = 10**9 + 7\n",
    "\n",
    "\n",
    "    dp = [[[0] * (max(rollMax) + 1) for _ in range(7)] for _ in range(N + 1)]\n",
    "    \n",
    "\n",
    "    for i in range(1, 7):\n",
    "        dp[1][i][1] = 1\n",
    "    \n",
    "    for n in range(2, N + 1):  \n",
    "        for i in range(1, 7):  \n",
    "            for k in range(1, rollMax[i - 1] + 1):  \n",
    "                for j in range(1, 7):\n",
    "                    if i != j:\n",
    "                        dp[n][i][1] = (dp[n][i][1] + dp[n - 1][j][k]) % max_val\n",
    "                \n",
    "\n",
    "                if k > 1:\n",
    "                    dp[n][i][k] = (dp[n][i][k] + dp[n - 1][i][k - 1]) % max_val\n",
    "\n",
    "    total_sequences = 0\n",
    "    for i in range(1, 7):\n",
    "        for k in range(1, rollMax[i - 1] + 1):\n",
    "            total_sequences = (total_sequences + dp[N][i][k]) % max_val\n",
    "\n",
    "    return total_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the pre-trained Wav2Vec2 processor and model\n",
    "model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name).to(\"cuda\")  # Move model to GPU if available\n",
    "\n",
    "path_name = 'audio/final'  # Path to the folder containing audio files\n",
    "files = os.listdir(path_name)\n",
    "\n",
    "# Hyperparameters for windowing\n",
    "window_size = 2  # Window size in seconds\n",
    "stride = 1  # Stride in seconds\n",
    "\n",
    "result = {}\n",
    "\n",
    "for file in tqdm(files):\n",
    "    audio_file = os.path.join(path_name, file)  # Create the full file path\n",
    "    file_id = file.split('.')[0]  # Extract file id without the extension\n",
    "    \n",
    "    # Load audio file using librosa, resample to 16 kHz (required by Wav2Vec2)\n",
    "    input_audio, sample_rate = librosa.load(audio_file, sr=16000)  # Load with target sample rate\n",
    "    total_duration = librosa.get_duration(y=input_audio, sr=sample_rate)  # Get duration of the audio\n",
    "    \n",
    "    # Compute number of windows\n",
    "    window_length = window_size * sample_rate  # Number of samples in each window\n",
    "    stride_length = stride * sample_rate  # Number of samples to move between windows\n",
    "    \n",
    "    # Split audio into windows\n",
    "    windows = []\n",
    "    for start in range(0, len(input_audio) - window_length + 1, stride_length):\n",
    "        end = start + window_length\n",
    "        windows.append(input_audio[start:end])\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    # Process each window\n",
    "    for window in windows:\n",
    "        # Extract features using Wav2Vec2Processor\n",
    "        inputs = processor(window, return_tensors=\"pt\", sampling_rate=16000)  # Return PyTorch tensors\n",
    "        inputs = inputs.to(\"cuda\")  # Move to GPU\n",
    "        \n",
    "        # Extract features using the Wav2Vec2 model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs.input_values)  # Forward pass\n",
    "            \n",
    "        # Get the mean-pooled embedding for this window\n",
    "        window_embedding = torch.mean(outputs.last_hidden_state, dim=1).squeeze().to('cpu').tolist()\n",
    "        embeddings.append(window_embedding)\n",
    "\n",
    "    # Optionally: Mean pool across all window embeddings for a global feature vector\n",
    "    mean_pooled_embedding = np.mean(embeddings, axis=0).tolist()\n",
    "    \n",
    "    # Store the result for this audio file\n",
    "    result[file_id] = mean_pooled_embedding\n",
    "\n",
    "# Save the extracted features as a JSON file\n",
    "with open('audio_features.json', 'w') as f:\n",
    "    json.dump(result, f, indent=4)\n",
    "\n",
    "print(\"Audio features successfully extracted and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EmotionModel were not initialized from the model checkpoint at audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 690/690 [05:46<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Wav2Vec2BertModel\n",
    "from torch import nn\n",
    "\n",
    "class RegressionHead(nn.Module):\n",
    "    r\"\"\"Classification head.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "\n",
    "        return x\n",
    "# Load the pre-trained Wav2Vec 2.0 processor and model\n",
    "path_name = 'audio/final'\n",
    "files = os.listdir(path_name)\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2PreTrainedModel,\n",
    ")\n",
    "\n",
    "def process_func(\n",
    "    x: np.ndarray,\n",
    "    sampling_rate: int,\n",
    "    embeddings: bool = False,\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"Predict emotions or extract embeddings from raw audio signal.\"\"\"\n",
    "\n",
    "    # run through processor to normalize signal\n",
    "    # always returns a batch, so we just get the first entry\n",
    "    # then we put it on the device\n",
    "    y = processor(x, sampling_rate=sampling_rate)\n",
    "    y = y['input_values'][0]\n",
    "    y = y.reshape(1, -1)\n",
    "    y = torch.from_numpy(y).to('cuda')\n",
    "\n",
    "    # run through model\n",
    "    with torch.no_grad():\n",
    "        y = model(y)[0 if embeddings else 1]\n",
    "\n",
    "    # convert to numpy\n",
    "    y = y.detach().cpu().numpy()\n",
    "\n",
    "    return y\n",
    "\n",
    "class EmotionModel(Wav2Vec2PreTrainedModel):\n",
    "    r\"\"\"Speech emotion classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = RegressionHead(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "    ):\n",
    "\n",
    "        outputs = self.wav2vec2(input_values)\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = torch.mean(hidden_states, dim=1)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        return hidden_states, logits\n",
    "# Load the pre-trained Wav2Vec2 processor and model\n",
    "model_name = 'audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim'\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = EmotionModel.from_pretrained(model_name).to('cuda')\n",
    "\n",
    "result = {}\n",
    "for file in tqdm(files):\n",
    "    # Load a waveform using torchaudio, normalize the audio signal and extract the feature\n",
    "    audio_file = f\"{path_name}/{file}\"\n",
    "    file_id = file.split('.')[0]\n",
    "    input_audio, sample_rate = librosa.load(audio_file,  sr=16000)\n",
    "\n",
    "    y = processor(input_audio, sampling_rate=16000)\n",
    "    y = y['input_values'][0]\n",
    "    y = y.reshape(1, -1)\n",
    "    y = torch.from_numpy(y).to('cuda')\n",
    "\n",
    "    # run through model\n",
    "    with torch.no_grad():\n",
    "        y = model(y)[0 if True else 1]\n",
    "\n",
    "    # convert to numpy\n",
    "    y = y.detach().cpu().numpy()\n",
    "    result[file_id] = y[0].tolist()\n",
    "\n",
    "with open('audio_features_wav2vec2_bert.json', 'w') as f:\n",
    "    json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
