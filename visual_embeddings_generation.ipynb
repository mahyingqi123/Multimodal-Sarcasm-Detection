{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yingq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# This file is to generate embeddings from images\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import open_clip\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 690/690 [1:30:53<00:00,  7.90s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import TimesformerModel, TimesformerConfig\n",
    "from torchvision import transforms\n",
    "\n",
    "# Initialize the TimeSformer model\n",
    "model = TimesformerModel.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define video preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def load_frames_from_folder(folder):\n",
    "    frames = []\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        frames.append(preprocess(img))\n",
    "    return frames\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 4  # Reduced batch size due to model size\n",
    "num_frames = 8  # TimeSformer typically uses 8 or 16 frames\n",
    "\n",
    "path_to_folders = 'video/Frames/'\n",
    "folders = os.listdir(path_to_folders)\n",
    "result = {}\n",
    "\n",
    "for frames_folder in tqdm(folders):\n",
    "    folder_path = os.path.join(path_to_folders, frames_folder)\n",
    "    frames = load_frames_from_folder(folder_path)\n",
    "    \n",
    "    # Process video in segments of num_frames\n",
    "    segments = []\n",
    "    for i in range(0, len(frames), num_frames):\n",
    "        segment = frames[i:i + num_frames]\n",
    "        \n",
    "        # If segment is shorter than num_frames, pad with zeros\n",
    "        if len(segment) < num_frames:\n",
    "            padding = [torch.zeros_like(segment[0]) for _ in range(num_frames - len(segment))]\n",
    "            segment.extend(padding)\n",
    "        \n",
    "        # Stack frames for TimeSformer input [batch, num_frames, channels, height, width]\n",
    "        segment_tensor = torch.stack(segment).unsqueeze(0)\n",
    "        segments.append(segment_tensor)\n",
    "    \n",
    "    # Process segments in batches\n",
    "    all_features = []\n",
    "    for i in range(0, len(segments), batch_size):\n",
    "        batch_segments = segments[i:i + batch_size]\n",
    "        if len(batch_segments) > 0:\n",
    "            # Stack segments into a batch\n",
    "            batch = torch.cat(batch_segments, dim=0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # TimeSformer expects input shape: [batch_size, num_frames, channels, height, width]\n",
    "                # Rearrange dimensions if needed\n",
    "                batch = batch.permute(0, 1, 2, 3, 4)\n",
    "                \n",
    "                # Get features\n",
    "                outputs = model(batch, output_hidden_states=True)\n",
    "                # Use the final hidden state as features\n",
    "                features = outputs.last_hidden_state.mean(dim=1)  # Average over sequence length\n",
    "                all_features.append(features.cpu())\n",
    "    \n",
    "    # Concatenate all features for this video\n",
    "    if all_features:\n",
    "        video_features = torch.cat(all_features, dim=0)\n",
    "        # Average all segment features to get video-level representation\n",
    "        video_features = torch.mean(video_features, dim=0)\n",
    "        result[frames_folder] = video_features.numpy().tolist()\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save results\n",
    "import json\n",
    "with open(\"video_features_timesformer.json\", \"w\") as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yingq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\yingq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\yingq/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:17<00:00, 5.71MB/s]\n",
      "100%|██████████| 690/690 [38:15<00:00,  3.33s/it] \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 94\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Optionally, save 'result' to a file using pickle or json\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_embeddings_resnet.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 94\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mdump(result, f)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence  # For padding sequences\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# Set up device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the pre-trained ResNet-50 model and remove the final classification layer\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "# Remove the last fully-connected layer so that we get features\n",
    "resnet_model = torch.nn.Sequential(*(list(resnet_model.children())[:-1]))\n",
    "resnet_model.to(device)\n",
    "resnet_model.eval()\n",
    "\n",
    "# Define preprocessing transform for ResNet\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet means\n",
    "        std=[0.229, 0.224, 0.225]    # ImageNet stds\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Path to the folders containing frames (each folder represents one video)\n",
    "path_to_folders = 'video/Frames/'\n",
    "folders = os.listdir(path_to_folders)\n",
    "\n",
    "# Function to load frames from a folder in order\n",
    "def load_frames_from_folder(folder):\n",
    "    frames = []\n",
    "    for filename in sorted(os.listdir(folder)):  # Sorting ensures temporal order\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = Image.open(img_path).convert('RGB')  # Ensure 3-channel image\n",
    "        frames.append(img)\n",
    "    return frames\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16   # Adjust based on your GPU capacity\n",
    "window_size = 16  # Number of frames per temporal window\n",
    "\n",
    "# Dictionary to store the result\n",
    "result = {}\n",
    "\n",
    "# Process each folder (video)\n",
    "for frames_folder in tqdm(folders):\n",
    "    folder_path = os.path.join(path_to_folders, frames_folder)\n",
    "    frames = load_frames_from_folder(folder_path)\n",
    "    \n",
    "    # Preprocess frames using the defined transform and move them to device\n",
    "    preprocessed_frames = [preprocess(frame).to(device) for frame in frames]\n",
    "    \n",
    "    # Split frames into windows of size `window_size`\n",
    "    windows = [preprocessed_frames[i:i + window_size] for i in range(0, len(preprocessed_frames), window_size)]\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for window in windows:\n",
    "            # Stack the frames in the window; shape: (window_length, 3, H, W)\n",
    "            window_tensor = torch.stack(window)\n",
    "            window_dataset = TensorDataset(window_tensor)\n",
    "            window_loader = DataLoader(window_dataset, batch_size=batch_size)\n",
    "            \n",
    "            window_embeddings = []\n",
    "            for batch in window_loader:\n",
    "                batch_frames = batch[0].to(device)  # Batch of frames\n",
    "                # Forward pass through ResNet; output shape: (batch_size, 2048, 1, 1)\n",
    "                features = resnet_model(batch_frames)\n",
    "                # Flatten the output to shape: (batch_size, 2048)\n",
    "                features = features.view(features.size(0), -1)\n",
    "                window_embeddings.append(features.cpu())\n",
    "            \n",
    "            # Aggregate embeddings in the window (e.g., using mean pooling)\n",
    "            window_embedding = torch.mean(torch.vstack(window_embeddings), dim=0)\n",
    "            embeddings.append(window_embedding)\n",
    "    \n",
    "    # Stack all window embeddings for the current folder (video)\n",
    "    embeddings_tensor = torch.stack(embeddings)  # Shape: (num_windows, 2048)\n",
    "    \n",
    "    \n",
    "    # Save the result for the current folder\n",
    "    result[frames_folder] = embeddings_tensor.cpu().numpy().tolist()\n",
    "\n",
    "# Optionally, save 'result' to a file using pickle or json\n",
    "\n",
    "with open('video_embeddings_resnet.json', 'w') as f:\n",
    "    json.dump(result, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
